{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS-BOnNfYstU"
      },
      "outputs": [],
      "source": [
        "!pip install pymupdf\n",
        "!pip install TTS==0.14.0\n",
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Awn0Og_DzDU6"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import re\n",
        "from TTS.api import TTS\n",
        "from pydub import AudioSegment\n",
        "import time\n",
        "import nltk\n",
        "nltk.download('punkt')  # Download the necessary resource for tokenization\n",
        "\n",
        "\n",
        "voice = \"/content/w.wav\"\n",
        "pdf_file_path = \"/content/My Inventions Nikola Tesla.pdf\"\n",
        "\n",
        "\n",
        "G = True\n",
        "clone = TTS(model_name=\"tts_models/multilingual/multi-dataset/your_tts\", progress_bar=False, gpu=G)\n",
        "\n",
        "language = \"en\"\n",
        "output_audio_files = []  # To store paths of individual audio files\n",
        "contents_to_read = []  # To store contents of the table of contents\n",
        "default_texts = [\n",
        "        \"The audio you are currently experiencing has been generated using proprietary technology from Tesla dot AI and Coqui, licensed solely for non-commercial purposes. Any unlawful use or distribution of this audio, including its incorporation into illegal activities such as pirated books or any other illegal content, is strictly prohibited and may result in legal consequences.\",  # Non-commercial license statement\n",
        "        \"Blind Owl\",  # Replace with the book's name\n",
        "        \"by Sadegh Hedayat\",  # Replace with the author's name\n",
        "        ]\n",
        "def Zero_Shot_CloneTTS(voice, language, text):\n",
        "    output_path = f\"output_{len(output_audio_files)}.wav\"\n",
        "    clone.tts_to_file(text=text,\n",
        "                      speaker_wav=voice,\n",
        "                      language=language,\n",
        "                      file_path=output_path)\n",
        "    output_audio_files.append(output_path)\n",
        "\n",
        "contents_to_read = []\n",
        "def table_content(pdf_path):\n",
        "    sentences_continued = ''\n",
        "    pdf_document = fitz.open(pdf_path)\n",
        "\n",
        "    # Process only the first 10 pages\n",
        "    num_pages_to_process = min(10, pdf_document.page_count)\n",
        "\n",
        "    for page_number in range(num_pages_to_process):\n",
        "        page = pdf_document.load_page(page_number)\n",
        "\n",
        "        text = page.get_text()\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Split text into sentences using NLTK's sent_tokenize\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "        # Count complete sentences to determine \"normal\" pages\n",
        "        complete_sentence_count = sum(1 for sentence in sentences if sentence.strip() and re.search(r'\\w', sentence.strip()))\n",
        "\n",
        "        toc_keywords = [\"Contents\", \"Table of Contents\",\"Copyright\",\"All rights reserved\"]  # Define your TOC keywords here\n",
        "\n",
        "\n",
        "        potential_toc_text = ' '.join(sentences)\n",
        "        pdf_document.delete_page(page_number)\n",
        "        for keyword in toc_keywords:\n",
        "            if keyword in potential_toc_text:\n",
        "                print(f\"Page {page_number + 1} might contain the table of contents:\")\n",
        "                content_started = False\n",
        "                for sentence in sentences:\n",
        "                    if content_started and re.search(r'\\w', sentence.strip()):\n",
        "                        contents_to_read.append(sentence.strip())  # Add contents to the list\n",
        "                    elif re.search(r'\\b' + re.escape(keyword) + r'\\b', sentence.strip()):\n",
        "                        content_started = True\n",
        "                    elif content_started and not re.search(r'\\w', sentence.strip()):\n",
        "                        break\n",
        "                break\n",
        "\n",
        "        pdf_document.delete_page(page_number)\n",
        "\n",
        "    pdf_document.save(\"modified_document.pdf\")\n",
        "    pdf_document.close()\n",
        "def extract_and_tokenize_text(pdf_path):\n",
        "    sentences_continued = ''\n",
        "    pdf_document = fitz.open(pdf_path)\n",
        "\n",
        "    # Process only the first 10 pages\n",
        "    num_pages_to_process = min(10, pdf_document.page_count)\n",
        "\n",
        "    for page_number in range(num_pages_to_process):\n",
        "        page = pdf_document.load_page(page_number)\n",
        "\n",
        "        text = page.get_text()\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Split text into sentences using NLTK's sent_tokenize\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        complete_sentence_count = sum(1 for sentence in sentences if sentence.strip() and re.search(r'\\w', sentence.strip()))\n",
        "        start_time = time.time()  # Record start time before processing the page\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if sentence:\n",
        "                Zero_Shot_CloneTTS(voice, language, sentence)\n",
        "\n",
        "        end_time = time.time()  # Record end time after processing the page\n",
        "        elapsed_time = end_time - start_time\n",
        "        print(f\"Page {page_number + 1} processed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "    pdf_document.close()\n",
        "\n",
        "\n",
        "for i in default_texts:\n",
        "  Zero_Shot_CloneTTS(voice, language, i)\n",
        "\n",
        "\n",
        "\n",
        "table_content(pdf_file_path)\n",
        "# for k in contents_to_read:\n",
        "#   Zero_Shot_CloneTTS(voice, language, k)\n",
        "\n",
        "extract_and_tokenize_text(\"modified_document.pdf\")\n",
        "\n",
        "\n",
        "# Combine all generated audio files into a single file\n",
        "combined_audio = AudioSegment.empty()\n",
        "for audio_file in output_audio_files:\n",
        "    audio_segment = AudioSegment.from_wav(audio_file)\n",
        "    combined_audio += audio_segment\n",
        "\n",
        "output_combined_audio = \"combined_output.wav\"\n",
        "combined_audio.export(output_combined_audio, format=\"wav\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
